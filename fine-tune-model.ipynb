{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b39c19-19b6-4a9f-a58c-7b6747897289",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac479b4b-81be-473f-b762-79e42018e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models, losses, InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd1f1c92-2990-4a8e-a91b-5306b35a9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3625e741-37b8-4238-a7e0-be4fa64751f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import LightningModule, LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb7dafe-36d4-43bd-a955-4e0499cc7a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418268e768ec461583216b2853521950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc123333e28c4487810cea1f27364015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e14ed918934fc2bb4fbddff7b1d9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e953450b503406a96a19cc7ac1a9102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ff11f163864446955d4fffd2d7ec1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the embedding model with a pre-trained Transformer\n",
    "word_embedding_model = models.Transformer('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369df90f-b0bf-43b8-a790-3482d81789c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pooling model to aggregate token embeddings into a single sentence embedding\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28be1c4d-0ed2-42d5-80e3-d2390be7e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them into a SentenceTransformer model\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c65f968c-b7e7-4cfe-803e-157d4be729b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For simplicity, each item is just a sentence here. Adapt as needed.\n",
    "        return InputExample(texts=[self.sentences[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809abf6-c64f-45a6-90c4-353899ddcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, sentences, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.sentences = sentences\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Split your dataset here if necessary\n",
    "        # For demonstration, we're treating all sentences as part of a single dataset\n",
    "        self.dataset = SentenceDataset(self.sentences)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b5212-29a4-464a-85b2-8f82f16194d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbeddingsModel(LightningModule):\n",
    "    def __init__(self, model_name='bert-base-uncased', learning_rate=2e-5):\n",
    "        super().__init__()\n",
    "        # Using a pre-trained model from Hugging Face and adapting it for sentence embeddings\n",
    "        word_embedding_model = models.Transformer(model_name)\n",
    "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "        self.model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        return self.model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sentences = [example.texts[0] for example in batch]\n",
    "        embeddings = self.forward(sentences)\n",
    "        # Define your loss function here. For simplicity, this part is abstracted.\n",
    "        loss = torch.tensor(0)  # Placeholder for actual loss calculation\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe4f58-d85c-42b2-9e0f-3254c0c9a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "class SentenceClassificationModel(pl.LightningModule):\n",
    "    def __init__(self, sentence_classifier):\n",
    "        super(SentenceClassificationModel, self).__init__()\n",
    "        self.sentence_classifier = sentence_classifier\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        logits = self.sentence_classifier(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        loss, logits = self(input_ids, attention_mask, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=5e-5)\n",
    "\n",
    "# # Assuming `sentence_classifier` is an instance of `SentenceClassifier` with your model\n",
    "# model = SentenceClassificationModel(sentence_classifier)\n",
    "\n",
    "# # Prepare your data loaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Train the model\n",
    "# trainer = pl.Trainer(max_epochs=3, gpus=1 if torch.cuda.is_available() else 0)\n",
    "# trainer.fit(model, train_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
